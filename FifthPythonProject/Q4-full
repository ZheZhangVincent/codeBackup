###########################################################
## Q4 Answer: algorithm part: with epsilon (y is reward) ##
###########################################################

'''this part is Q-learn with epsilon to collected data (y is reward)
'''
##get a loop to form epsilon number
epsilon = 0.1

##create two list to save result
q_epis_list = []
q_epis_r_list = []

while epsilon <= 0.7:
    env = get_windy_grid_water(0, -100)
    agent = QLearn(0.1, 0.99, epsilon, env.num_states, env.actions)

    ##first of all, when Q-Learning algorithm is not convergence
    while not agent.convergence:
        
        ##create a count number to update episode length in batch
        count = 0
        
        ##used to save episode lengths in 200 times
        patch_lengths = 0.0
        while count < 200:
            count = count + 1
            
            ##create time_step to save episode length
            time_step = 0
            
            ##initialize s
            state = env.pos_to_state(env.init_pos)
            
            ##repeat until achieve the goal pos
            while not env.end_of_episode():
                
                ##choose an action from Q based on state
                action = agent.select_action(state)
                
                ##take action, and get reward, new state, new position, add one time_step
                result = env.generate(action)
                reward = result[-1]
                new_state = result[0]

                ##update Q
                agent.update_Q(state, action, new_state, reward)
                
                ##assign new_stae to state
                state = int(new_state)
                
                ##add the time_step
                time_step = time_step + 1
                
            ##test the algorithm is convergence or not
            patch_lengths = patch_lengths + time_step
        
        ##get the average episode lengths in patch
        patch_lengths = float(patch_lengths / 200)

        ##test the algorithm is convergence or not
        agent.test_convergence(patch_lengths)
        
        ##if not convergence, make the current patch_length to past_episode
        if not agent.convergence:
            agent.past_episode = float(patch_lengths)
    
    ##when the algorithm convergence collected data
    if agent.convergence:
        
        ##create episode to act as count to 500
        episode = 0
        
        ##create sum_time to save total time size of 500 times episodes
        sum_time = 0.0
        
        while episode < 500:
            episode = episode + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            
            ##there is no need to update Q, because algorithm is convergence
            while not env.end_of_episode():
                action = agent.select_action(state)
                new_state = env.generate(action)[0]
                state = int(new_state)
                time_step = time_step + 1
            sum_time = sum_time + time_step
            
        ##get the average of 500 episodes
        episode_length = sum_time / 500

    
    ##add these delta and averge_episode_length information into former list
    q_epis_list.append(epsilon)
    
    ##get the average reward
    average_reward = agent.get_reward() / len(agent.Q)

    q_epis_r_list.append(average_reward)
    
    ##when finish collecting data with 500 times episodes, add 0.1 to epsilon
    epsilon = epsilon + 0.1

'''this part is perform Sarsa, with epsilon to collect data, majority code
is same when compare with Q3, then I decide to delete some comments to save
reading time. (y is reward)
'''
    
epsilon = 0.1
s_epis_list = []
s_epis_r_list = []

while epsilon <= 0.7:
    env = get_windy_grid_water(0, -100)
    agent = Sarsa(0.1, 0.99, epsilon, env.num_states, env.actions)

    while not agent.convergence:

        count = 0
        patch_lengths = 0.0
        while count < 200:
            count = count + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            action = agent.select_action(state)
            while not env.end_of_episode():
                
                result = env.generate(action)
                reward = result[-1]
                new_state = result[0]
                new_action = agent.select_action(new_state)
                agent.update_Q(state, action, new_state, new_action, reward)
                action = str(new_action)
                state = int(new_state)
                time_step = time_step + 1
                
            patch_lengths = patch_lengths + time_step
            
        patch_lengths = float(patch_lengths / 200)
        agent.test_convergence(patch_lengths)
        
        if not agent.convergence:
            agent.past_episode = float(patch_lengths)
        
    ##when the algorithm convergence
    if agent.convergence:
        
        episode = 0
        sum_time = 0.0
        while episode < 500:
            episode = episode + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            
            while not env.end_of_episode():
                action = agent.select_action(state)
                new_state = env.generate(action)[0]
                state = int(new_state)
                time_step = time_step + 1
            sum_time = sum_time + time_step
        episode_length = sum_time / 500

    s_epis_list.append(epsilon)
    average_reward = agent.get_reward() / len(agent.Q)

    s_epis_r_list.append(average_reward)

    epsilon = epsilon + 0.1


###############################################################
## Q4 Answer: graph display part: with epsilon (y is reward) ##
###############################################################

import matplotlib.pyplot as plt
plt.title("Q4 Graphic with epsilon")
plt.ylabel("Average Reward")
plt.xlabel("Epsilon")

##add the information into x-label and y-label, and show
plt.plot(q_epis_list,q_epis_r_list, label="Q-Learning: Epsilon' function of Reward")
plt.plot(s_epis_list,s_epis_r_list, label="Sarsa: Epsilon' function of Reward")

plt.legend(loc='lower right')
plt.show()




##############################################################
## Q4 Answer: algorithm part: without epsilon (y is reward) ##
##############################################################

'''this part is Q-learn without epsilon to collected data (y is reward)
'''
##get a loop to form epsilon number
epsilon = 0.1

##create two list to save result
q_epis_list_without = []
q_epis_r_list_without = []

while epsilon <= 0.7:
    env = get_windy_grid_water(0, -100)
    agent = QLearn(0.1, 0.99, epsilon, env.num_states, env.actions)

    while not agent.convergence:
        count = 0
        patch_lengths = 0.0
        while count < 200:
            count = count + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            while not env.end_of_episode():
                
                action = agent.select_action(state)
                result = env.generate(action)
                reward = result[-1]
                new_state = result[0]
                agent.update_Q(state, action, new_state, reward)
                state = int(new_state)
                
                time_step = time_step + 1
            patch_lengths = patch_lengths + time_step
        patch_lengths = float(patch_lengths / 200)

        agent.test_convergence(patch_lengths)
        
        if not agent.convergence:
            agent.past_episode = float(patch_lengths)
            
    if agent.convergence:
        agent.epsilon = 0
        episode = 0
        sum_time = 0.0
        
        while episode < 500:
            episode = episode + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            while not env.end_of_episode():
                action = agent.select_action(state)
                new_state = env.generate(action)[0]
                state = int(new_state)
                time_step = time_step + 1
            sum_time = sum_time + time_step
        episode_length = sum_time / 500
        
    q_epis_list_without.append(epsilon)
    
    average_reward = agent.get_reward() / len(agent.Q)
    q_epis_r_list_without.append(average_reward)
    
    epsilon = epsilon + 0.1
    
'''this part is perform Sarsa, without epsilon to collect data
'''
    
epsilon = 0.1
s_epis_list_without = []
s_epis_r_list_without = []

while epsilon <= 0.7:
    env = get_windy_grid_water(0, -100)
    agent = Sarsa(0.1, 0.99, epsilon, env.num_states, env.actions)

    while not agent.convergence:

        count = 0
        patch_lengths = 0.0
        while count < 200:
            count = count + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            action = agent.select_action(state)
            while not env.end_of_episode():
                
                result = env.generate(action)
                reward = result[-1]
                new_state = result[0]
                new_action = agent.select_action(new_state)
                agent.update_Q(state, action, new_state, new_action, reward)
                action = str(new_action)
                state = int(new_state)
                time_step = time_step + 1
                
            patch_lengths = patch_lengths + time_step
            
        patch_lengths = float(patch_lengths / 200)
        agent.test_convergence(patch_lengths)
        
        if not agent.convergence:
            agent.past_episode = float(patch_lengths)
        
    ##when the algorithm convergence
    if agent.convergence:
        agent.epsilon = 0
        episode = 0
        sum_time = 0.0
        while episode < 500:
            episode = episode + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            
            while not env.end_of_episode():
                action = agent.select_action(state)
                new_state = env.generate(action)[0]
                state = int(new_state)
                time_step = time_step + 1
            sum_time = sum_time + time_step
        episode_length = sum_time / 500

    s_epis_list_without.append(epsilon)
    average_reward = agent.get_reward() / len(agent.Q)

    s_epis_r_list_without.append(average_reward)
    
    epsilon = epsilon + 0.1

##################################################################
## Q4 Answer: graph display part: without epsilon (y is reward) ##
##################################################################

import matplotlib.pyplot as plt
plt.title("Q4 Graphic without epsilon")
plt.ylabel("Average Reward")
plt.xlabel("Epsilon")

##add the information into x-label and y-label, and show
plt.plot(q_epis_list_without,q_epis_r_list_without, label="Q-Learning: Epsilon' function of Reward")
plt.plot(s_epis_list_without,s_epis_r_list_without, label="Sarsa: Epsilon' function of Reward")

plt.legend(loc='lower right')
plt.show()


###################################################################
## Q4 Answer: algorithm part: with epsilon (y is episode_length) ##
###################################################################

'''this part is Q-learn with epsilon to collected data (y is episode_length)
'''
    
epsilon = 0.1
q_epis_list = []
q_epis_l_list = []

while epsilon <= 0.7:
    env = get_windy_grid_water(0, -100)
    agent = QLearn(0.1, 0.99, epsilon, env.num_states, env.actions)
    while not agent.convergence:
        count = 0
        patch_lengths = 0.0
        while count < 200:
            count = count + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            while not env.end_of_episode():
                action = agent.select_action(state)
                result = env.generate(action)
                reward = result[-1]
                new_state = result[0]
                agent.update_Q(state, action, new_state, reward)
                state = int(new_state)
                time_step = time_step + 1
            
            patch_lengths = patch_lengths + time_step
        
        patch_lengths = float(patch_lengths / 200)
        
        agent.test_convergence(patch_lengths)
        if not agent.convergence:
            agent.past_episode = float(patch_lengths)
            
    if agent.convergence:
        episode = 0
        sum_time = 0.0
        
        while episode < 500:
            episode = episode + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            while not env.end_of_episode():
                action = agent.select_action(state)
                new_state = env.generate(action)[0]
                state = int(new_state)
                time_step = time_step + 1
            sum_time = sum_time + time_step
        episode_length = sum_time / 500
        
    q_epis_list.append(epsilon)
    
    q_epis_l_list.append(episode_length)

    epsilon = epsilon + 0.1
    
'''this part is perform Sarsa, with epsilon to collect data, majority code
is same when compare with Q3, then I decide to delete some comments to save
reading time. (y is episode_length)
'''
    
epsilon = 0.1
s_epis_list = []
s_epis_l_list = []

while epsilon <= 0.7:
    env = get_windy_grid_water(0, -100)
    agent = Sarsa(0.1, 0.99, epsilon, env.num_states, env.actions)

    while not agent.convergence:

        count = 0
        patch_lengths = 0.0
        while count < 200:
            count = count + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            action = agent.select_action(state)
            while not env.end_of_episode():
                
                result = env.generate(action)
                reward = result[-1]
                new_state = result[0]
                new_action = agent.select_action(new_state)
                agent.update_Q(state, action, new_state, new_action, reward)
                action = str(new_action)
                state = int(new_state)
                time_step = time_step + 1
                
            patch_lengths = patch_lengths + time_step
            
        patch_lengths = float(patch_lengths / 200)
        agent.test_convergence(patch_lengths)
        
        if not agent.convergence:
            agent.past_episode = float(patch_lengths)
        
    ##when the algorithm convergence
    if agent.convergence:
        
        episode = 0
        sum_time = 0.0
        while episode < 500:
            episode = episode + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            
            while not env.end_of_episode():
                action = agent.select_action(state)
                new_state = env.generate(action)[0]
                state = int(new_state)
                time_step = time_step + 1
            sum_time = sum_time + time_step
        episode_length = sum_time / 500

    s_epis_list.append(epsilon)
    s_epis_l_list.append(episode_length)
    
    
    epsilon = epsilon + 0.1


#######################################################################
## Q4 Answer: graph display part: with epsilon (y is Episode Length) ##
#######################################################################

import matplotlib.pyplot as plt
plt.title("Q4 Graphic with epsilon")
plt.ylabel("Average Episode Length")
plt.xlabel("Epsilon")

##add the information into x-label and y-label, and show
plt.plot(q_epis_list,q_epis_l_list, label="Q-Learning: Epsilon' function of Episode Length")
plt.plot(s_epis_list,s_epis_l_list, label="Sarsa: Epsilon' function of Episode Length")

plt.legend(loc='lower right')
plt.show()


######################################################################
## Q4 Answer: algorithm part: without epsilon (y is episode_length) ##
######################################################################

'''this part is Q-learn without epsilon to collected data (y is episode_length)
'''
##get a loop to form epsilon number
epsilon = 0.1

##create two list to save result
q_epis_list_without = []
q_epis_l_list_without = []

while epsilon <= 0.7:
    env = get_windy_grid_water(0, -100)
    agent = QLearn(0.1, 0.99, epsilon, env.num_states, env.actions)

    while not agent.convergence:
        count = 0
        patch_lengths = 0.0
        while count < 200:
            count = count + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            while not env.end_of_episode():
                
                action = agent.select_action(state)
                result = env.generate(action)
                reward = result[-1]
                new_state = result[0]
                agent.update_Q(state, action, new_state, reward)
                state = int(new_state)
                
                time_step = time_step + 1
            patch_lengths = patch_lengths + time_step
        patch_lengths = float(patch_lengths / 200)
        
        agent.test_convergence(patch_lengths)
        
        if not agent.convergence:
            agent.past_episode = float(patch_lengths)
            
    if agent.convergence:
        agent.epsilon = 0
        episode = 0
        sum_time = 0.0
        
        while episode < 500:
            episode = episode + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            while not env.end_of_episode():
                action = agent.select_action(state)
                new_state = env.generate(action)[0]
                state = int(new_state)
                time_step = time_step + 1
            sum_time = sum_time + time_step
        episode_length = sum_time / 500
        
    q_epis_list_without.append(epsilon)
    
    q_epis_l_list_without.append(episode_length)
    
    epsilon = epsilon + 0.1
    
'''this part is perform Sarsa, without epsilon to collect data
'''
    
epsilon = 0.1
s_epis_list_without = []
s_epis_l_list_without = []

while epsilon <= 0.7:
    env = get_windy_grid_water(0, -100)
    agent = Sarsa(0.1, 0.99, epsilon, env.num_states, env.actions)

    while not agent.convergence:

        count = 0
        patch_lengths = 0.0
        while count < 200:
            count = count + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            action = agent.select_action(state)
            while not env.end_of_episode():
                
                result = env.generate(action)
                reward = result[-1]
                new_state = result[0]
                new_action = agent.select_action(new_state)
                agent.update_Q(state, action, new_state, new_action, reward)
                action = str(new_action)
                state = int(new_state)
                time_step = time_step + 1
                
            patch_lengths = patch_lengths + time_step
            
        patch_lengths = float(patch_lengths / 200)
        agent.test_convergence(patch_lengths)
        
        if not agent.convergence:
            agent.past_episode = float(patch_lengths)
        
    ##when the algorithm convergence
    if agent.convergence:
        agent.epsilon = 0
        episode = 0
        sum_time = 0.0
        while episode < 500:
            episode = episode + 1
            time_step = 0
            state = env.pos_to_state(env.init_pos)
            
            while not env.end_of_episode():
                action = agent.select_action(state)
                new_state = env.generate(action)[0]
                state = int(new_state)
                time_step = time_step + 1
            sum_time = sum_time + time_step
        episode_length = sum_time / 500

    s_epis_list_without.append(epsilon)

    s_epis_l_list_without.append(episode_length)
    
    epsilon = epsilon + 0.1

##########################################################################
## Q4 Answer: graph display part: without epsilon (y is episode length) ##
##########################################################################

import matplotlib.pyplot as plt
plt.title("Q4 Graphic without epsilon")
plt.ylabel("Average Episode Length")
plt.xlabel("Epsilon")

##add the information into x-label and y-label, and show
plt.plot(q_epis_list_without,q_epis_l_list_without, label="Q-Learning: Epsilon' function of Episode Length")
plt.plot(s_epis_list_without,s_epis_l_list_without, label="Sarsa: Epsilon' function of Episode Length")

plt.legend(loc='lower right')
plt.show()
